{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- read_data\n",
      "train data shape:\t (1526659, 376)\n",
      "test data shape:\t (10, 375)\n",
      "train data shape:\t (1526659, 267)\n",
      "test data shape:\t (10, 266)\n",
      "---------- clean_data\n",
      "---------- data_to_train\n",
      "---------- training: ANN\n"
     ]
    }
   ],
   "source": [
    "%run data_to_train.ipynb\n",
    "\n",
    "print('---------- training: ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "    # Create multiple dense layers as specified by the hidden_units list\n",
    "    for i in range(len(hidden_units)): \n",
    "        x = tf.keras.layers.Dense(hidden_units[i], activation=None,kernel_regularizer=regularizers.l2(0.01))(x)  # Use None for activation here since we're applying it after batch norm\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)  # Notice dropout_rates[i + 1] because of initial dropout after input layer\n",
    "\n",
    "    # Output layer\n",
    "    x = tf.keras.layers.Dense(num_labels, activation=None,kernel_regularizer=regularizers.l2(0.01))(x)  # Use None for activation here since we're applying it after\n",
    "    out = tf.keras.layers.Activation('sigmoid')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n",
    "                  metrics=[tf.keras.metrics.AUC(name='AUC')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = 12345\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_path = './saved/NNModel.hdf5'\n",
    "learning_rate = 1e-3\n",
    "label_smoothing = 0.01\n",
    "batch_size = 32\n",
    "hidden_units = [64, 128, 64, 32]\n",
    "dropout_rates = [0.1, 0.1, 0.1,0.1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Training Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If your GPU is supporting you can leverage this code\n",
    "# from tensorflow.keras import mixed_precision\n",
    "# mixed_precision.set_global_policy('mixed_float16') # Set global data policy to mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40710/40711 [============================>.] - ETA: 0s - loss: 0.1768 - AUC: 0.7194\n",
      "Epoch 1: val_AUC improved from -inf to 0.65340, saving model to NNModel.hdf5\n",
      "40711/40711 [==============================] - 459s 11ms/step - loss: 0.1768 - AUC: 0.7194 - val_loss: 0.1256 - val_AUC: 0.6534 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "40710/40711 [============================>.] - ETA: 0s - loss: 0.1551 - AUC: 0.7448\n",
      "Epoch 2: val_AUC did not improve from 0.65340\n",
      "40711/40711 [==============================] - 452s 11ms/step - loss: 0.1551 - AUC: 0.7448 - val_loss: 589.7628 - val_AUC: 0.5260 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "40707/40711 [============================>.] - ETA: 0s - loss: 0.1547 - AUC: 0.7475\n",
      "Epoch 3: val_AUC did not improve from 0.65340\n",
      "40711/40711 [==============================] - 462s 11ms/step - loss: 0.1547 - AUC: 0.7475 - val_loss: 27102.5938 - val_AUC: 0.5264 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "40711/40711 [==============================] - ETA: 0s - loss: 0.1544 - AUC: 0.7471\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 4: val_AUC did not improve from 0.65340\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "40711/40711 [==============================] - 458s 11ms/step - loss: 0.1544 - AUC: 0.7471 - val_loss: 3623.8574 - val_AUC: 0.5294 - lr: 0.0010\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "model = create_deep_mlp(X_train.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "rlr = ReduceLROnPlateau(monitor='val_AUC', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='max')\n",
    "ckp = ModelCheckpoint(ckp_path, monitor='val_AUC', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_AUC', min_delta=1e-4, patience=3, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Model fitting\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                      epochs=100, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=[rlr, ckp, es], \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6998/6998 [==============================] - 70s 10ms/step - loss: 0.1180 - AUC: 0.7851\n",
      "Epoch 2/5\n",
      "6998/6998 [==============================] - 82s 12ms/step - loss: 0.1158 - AUC: 0.7925\n",
      "Epoch 3/5\n",
      "6998/6998 [==============================] - 73s 10ms/step - loss: 0.1149 - AUC: 0.7968\n",
      "Epoch 4/5\n",
      "6998/6998 [==============================] - 72s 10ms/step - loss: 0.1144 - AUC: 0.7981\n",
      "Epoch 5/5\n",
      "6998/6998 [==============================] - 76s 11ms/step - loss: 0.1141 - AUC: 0.7998\n",
      "1750/1750 [==============================] - 4s 2ms/step\n",
      "ROC AUC Score after fine-tuning: 0.8223442577685152\n"
     ]
    }
   ],
   "source": [
    "# Load the best weights\n",
    "model.load_weights(ckp_path)\n",
    "\n",
    "# Fine-tuning on validation data\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate / 100),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n",
    "              metrics=[tf.keras.metrics.AUC(name='AUC')])\n",
    "hisotry2 = model.fit(X_valid, y_valid, epochs=5, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Save the fine-tuned weights\n",
    "model.save_weights(ckp_path)\n",
    "\n",
    "# Evaluation\n",
    "# Here you should use your validation set or a separate test set\n",
    "predicted_valid = model.predict(X_valid, batch_size=batch_size * 4).ravel()\n",
    "auc_score = roc_auc_score(y_valid, predicted_valid)\n",
    "print(f'ROC AUC Score after fine-tuning: {auc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3040"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
